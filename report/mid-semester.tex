\documentclass[11pt,letterpaper]{article}

\usepackage[letterpaper,margin=0.8in,nohead]{geometry}

\usepackage[colorlinks]{hyperref}
\usepackage{url}
\usepackage{breakurl}

\hypersetup{
    colorlinks,
    linkcolor={red},
    citecolor={blue},
    urlcolor={blue}
}

% add packages as needed


\title{CIS 6930: Privacy \& Machine Learning\\
	\large Mid-Semester Project Report: Deep learning-based approaches for stylometric De-Anonymization for model authorship} %% TODO: replace with the title of your project

%% TODO: your name and email go here (all members of the group)
%% Comment out as needed and designate a point of contact
\author{
        Apoorv Chandurkar \\{\em (Point of Contact)} \\
        apoorvchandurkar@ufl.edu\\
        \and
        Kunwardeep Singh \\
        kunwardeep.singh@ufl.edu\\
}

% set the date to today
\date{\today}


\begin{document} % start document tag

\maketitle


%%% Remember: writing counts! (try to be clear and concise.)
%%% the mid-semester proposal should be about 1 page (in 11pt font)


%% TODO: write a few paragraphs detailing your progress so far.
%% Make sure to address the following:
%% - What have you done so far?
%% - Do you have any preliminary results, if so what are they?
%% - What are you going to do next?
%% - Are you on schedule? Do you need to reduce the scope?
%% - What do you expect you will have by the end of the semester?
%%
\section{Progress \& Preliminary Results}

% TODO:
Phase 1: Training data-set generation:
        \begin{enumerate}
                \item We picked the top 4 state of the art language models in the literature, 1. OpenAI GPT 2. GPT2 3. XL-NET 4. Salesforce CTRL. After looking and testing different open-source implementation of these models, we decided to use huggingface transformer library \cite{Wolf2019HuggingFacesTS} for selecting and generating text. We studied the Transformers library, which has the option to choose and fine-tune many hyperparameters at the text generation phase. We decided on many hyperparameters like no. of tokens generated by the model and input prompt given to the text.
                \item We discussed that to be consistent across different models, input prompt given to generate text should be the same for all models. This will help in defining the classification task as each model will produce text based on the same context in the same domain. Hence, our de-anonymization classifier will be incentivized to discriminate based on the content structure rather than only looking at the individual tokens of the text.
                \item After surveying many NLP based text datasets, we decided to use the OpinRank review dataset \cite{ganesan2012opinion}, which is a dataset of cars and hotels reviews by customers. We focus on hotel reviews for our task. Our motivation for working with the reviews, in general, is that potential misuse of large scale text-generation models can be to generate fake positive or negative reviews to manipulate ratings and consumer sentiment on social media. 
        \end{enumerate}
Phase 2: Classification task
        \begin{enumerate}
                \item For the initial prototype, we decided to perform a binary classification task between two models with similar architecture but different parameters, GPT -1 and GPT. We created a sample dataset of 100 model generated reviews given same initial input of hotel reviews in the city of Chicago. 
                \item We created a lstm neural-network with the following architecture to train a neural network to differentiate between two models. We achieved a test accuracy of 68\% and a training accuracy of 91\% for the initial training iteration. This indicated a high overfitting of the classifier. We concluded we needed more training data to sufficient comment on the performance as well as the generalization capacity of our lstm model. 
                \begin{verbatim}



Layer (type)                 Output Shape              Param #   
=================================================================
embedding_4 (Embedding)      (None, 50, 32)            160000    
_______________________
lstm_4 (LSTM)                (None, 100)               53200     
_______________________
dense_4 (Dense)              (None, 1)                 101       
=================================================================
Total params: 213,301
Trainable params: 213,301
Non-trainable params: 0
                \end{verbatim}
                \item Total training and text generation at large scale take a very long time on local machines, so we are using HiperGator for large scale text generation and training of classifiers, which should complete within a week.
        \end{enumerate}

\section{Future Work}
        \begin{enumerate}
                \item Currently, we are looking at different classification neural networks other than lstm, which can give better performance for this particular task.
                \item After creating sufficiently large datasets, we will train our classifier to obtain performance metrics like accuracy, precision, recall, etc. and check for overfitting. We are also investigating the relationship between the architecture of the text generation model and the de-anonymization success rate for that particular model.
                \item We are planning to do pairwise binary classifiers and all in one k class classifier to check is there any significant performance difference between specific pair of models.
                \item We are also planning to add human-generated text as an additional category and check how differentiating it is compared to other model-authored texts. 
        \end{enumerate}

\bibliographystyle{ieeetr}
\bibliography{References}

\end{document} % end tag of the document